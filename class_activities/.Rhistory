cur_dev <- new_dev
iter <- iter + 1
}
return(list("Iterations" = iter, "coefficients" = beta))
}
my_glm_1 <- function(X, y, family, max_iter = 50){
if(family == "gaussian"){
n <- nrow(X)
p <- ncol(X)
beta <- solve(t(X) %*% X) %*% t(X) %*% y
iter <- 1
se <- sqrt(diag(sum((y - X %*% beta)^2)/(n - p) * solve(t(X) %*% X)))
coefs <- data.frame(beta = beta,
se = se,
t_score = beta/se,
p_value = 2*pt(abs(beta/se), n-p,
lower.tail = F))
} else {
output <- fisher_scoring(X, y, family, max_iter)
beta <- output$coefficients
iter <- output$Iterations
se <- sqrt(diag(solve(I(X, beta, family))))
coefs <- data.frame(beta = beta,
se = se,
z_score = beta/se,
p_value = 2*pnorm(abs(beta/se),
lower.tail = F))
}
row.names(coefs)[1] <- "(Intercept)"
return(list("coefficients" = coefs,
"iterations" = iter,
"family" = family,
"deviance" = deviance(X, y, beta, family)))
}
my_glm_2 <- function(formula, family, data, max_iter = 50){
y <- data[[deparse(formula[[2]])]]
X <- cbind(1, as.matrix(data[all.vars(formula[[3]])]))
return(my_glm_1(X, y, family, max_iter))
}
set.seed(123)
n <- 200
x <- rnorm(n)
p <- exp(1 + x)/(1 + exp(1 + x))
y <- rbinom(n, 1, p)
df <- data.frame(y, x)
my_glm_2(y ~ x, family = "binomial", data = df)
library(palmerpenguins)
library(tidyverse)
df <- penguins |>
drop_na()
colnames(df)
df <- penguins |>
drop_na()
m1 <- my_glm_2(body_mass_g ~ flipper_length_mm + bill_length_mm,
family = "gaussian", data = df)
m1$coefficients
m1$iterations
m1$family
m1$deviance
lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = df)
lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = df) |> summary()
colnames(df)
head(df$sex)
df <- penguins |>
drop_na() |>
mutate(sex = ifelse(sex == "female", 1, 0))
m1 <- my_glm_2(sex ~ flipper_length_mm + bill_length_mm,
family = "binomial", data = df)
m1$coefficients
glm(sex ~ flipper_length_mm + bill_length_mm, family = binomial, data = df) |> summary()
m1$iterations
m1$family
m1$deviance
log_test <- function(X, y, family, max_iter = 50){
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Fisher Scoring while loop until convergence or max iterations is reached
while(iterations <= max_iter){
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
# Calculate the score
U <- t(X) %*% (y - mu_hat)
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
# Calculate deviance
deviance <- -2 * sum(dbinom(y, 1, mu_hat, log=T))
# Check to see if convergence was reached
if(deviance - prev_deviance < 0.001){
break
}
else{
# Reset features for next iteration
prev_deviance <- deviance
beta_hat <- beta_hat_new
}
}
# Calculate standard error of coefficients using the inverse Fisher information matrix
stand_errors <- sqrt(diag(solve(t(X) %*% W %*% X)))
# Calculate the z-statistics for our coefficients
z_stats <- beta_hat_new / stand_errors
# Calculate p-values for coefficients
p_vals <- 2 * pnorm(abs(z_stats), lower.tail = FALSE)
# Set up results DataFrame
results <- list(
coefficients = data.frame(
beta = beta_hat_new,
se = stand_errors,
z_score = z_stats,
p_value = p_vals),
iterations = iterations + 1,
family = family,
deviance = deviance
)
return (results)
}
log_test <- function(X, y, family, max_iter = 50){
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Fisher Scoring while loop until convergence or max iterations is reached
while(iterations <= max_iter){
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
# Calculate the score
U <- t(X) %*% (y - mu_hat)
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
# Calculate deviance
deviance <- -2 * sum(dbinom(y, 1, mu_hat, log=T))
# Check to see if convergence was reached
if(deviance - prev_deviance < 0.001){
break
}
else{
# Reset features for next iteration
prev_deviance <- deviance
beta_hat <- beta_hat_new
}
}
# Calculate standard error of coefficients using the inverse Fisher information matrix
stand_errors <- sqrt(diag(solve(t(X) %*% W %*% X)))
# Calculate the z-statistics for our coefficients
z_stats <- beta_hat_new / stand_errors
# Calculate p-values for coefficients
p_vals <- 2 * pnorm(abs(z_stats), lower.tail = FALSE)
# Set up results DataFrame
results <- list(
coefficients = data.frame(
beta = beta_hat_new,
se = stand_errors,
z_score = z_stats,
p_value = p_vals),
iterations = iterations + 1,
family = family,
deviance = deviance
)
return (results)
}
set.seed(123)
n <- 200
x <- rnorm(n)
p <- exp(1 + x)/(1 + exp(1 + x))
y <- rbinom(n, 1, p)
# create the design matrix
X <- cbind(1, x)
# fit the model
m1 <- log_test(X, y, "binomial")
# Model output
m1$coefficients
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
beta_hat
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
mu_hat
# Calculate the score
U <- t(X) %*% (y - mu_hat)
U
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
beta_hat_new
U <- function(X, y, beta, family){
if(family == "binomial"){
mu <- c(exp(X %*% beta)/(1 + exp(X %*% beta)))
} else {
mu <- c(exp(X %*% beta))
}
return(t(X) %*% (y - mu))
}
I <- function(X, beta, family){
if(family == "binomial"){
mu <- c(exp(X %*% beta)/(1 + exp(X %*% beta)))
W <- diag(mu*(1 - mu))
} else {
mu <- c(exp(X %*% beta))
W <- diag(mu)
}
return(t(X) %*% W %*% X)
}
beta_hat_new
beta_hat
beta_hat + solve(I(X, beta_hat, "binomial")) %*% U(X, y, beta_hat, "binomial")
log_test <- function(X, y, family, max_iter = 50){
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Fisher Scoring while loop until convergence or max iterations is reached
while(iterations <= max_iter){
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
# Calculate the score
U <- t(X) %*% (y - mu_hat)
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
mu_hat <- exp(X %*% beta_hat_new) / (1 + exp(X %*% beta_hat_new))
# Calculate deviance
deviance <- -2 * sum(dbinom(y, 1, mu_hat, log=T))
# Check to see if convergence was reached
if(deviance - prev_deviance < 0.001){
break
}
else{
# Reset features for next iteration
prev_deviance <- deviance
beta_hat <- beta_hat_new
}
}
# Calculate standard error of coefficients using the inverse Fisher information matrix
stand_errors <- sqrt(diag(solve(t(X) %*% W %*% X)))
# Calculate the z-statistics for our coefficients
z_stats <- beta_hat_new / stand_errors
# Calculate p-values for coefficients
p_vals <- 2 * pnorm(abs(z_stats), lower.tail = FALSE)
# Set up results DataFrame
results <- list(
coefficients = data.frame(
beta = beta_hat_new,
se = stand_errors,
z_score = z_stats,
p_value = p_vals),
iterations = iterations + 1,
family = family,
deviance = deviance
)
return (results)
}
set.seed(123)
n <- 200
x <- rnorm(n)
p <- exp(1 + x)/(1 + exp(1 + x))
y <- rbinom(n, 1, p)
# create the design matrix
X <- cbind(1, x)
# fit the model
m1 <- log_test(X, y, "binomial")
# Model output
m1$coefficients
deviance
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
# Calculate the score
U <- t(X) %*% (y - mu_hat)
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
mu_hat <- exp(X %*% beta_hat_new) / (1 + exp(X %*% beta_hat_new))
# Calculate deviance
deviance <- -2 * sum(dbinom(y, 1, mu_hat, log=T))
deviance
deviance - prev_deviance
log_test <- function(X, y, family, max_iter = 50){
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Fisher Scoring while loop until convergence or max iterations is reached
while(iterations <= max_iter){
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
# Calculate the score
U <- t(X) %*% (y - mu_hat)
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
mu_hat <- exp(X %*% beta_hat_new) / (1 + exp(X %*% beta_hat_new))
# Calculate deviance
deviance <- -2 * sum(dbinom(y, 1, mu_hat, log=T))
# Check to see if convergence was reached
if(abs(deviance - prev_deviance) < 0.001){
break
}
else{
# Reset features for next iteration
prev_deviance <- deviance
beta_hat <- beta_hat_new
}
}
# Calculate standard error of coefficients using the inverse Fisher information matrix
stand_errors <- sqrt(diag(solve(t(X) %*% W %*% X)))
# Calculate the z-statistics for our coefficients
z_stats <- beta_hat_new / stand_errors
# Calculate p-values for coefficients
p_vals <- 2 * pnorm(abs(z_stats), lower.tail = FALSE)
# Set up results DataFrame
results <- list(
coefficients = data.frame(
beta = beta_hat_new,
se = stand_errors,
z_score = z_stats,
p_value = p_vals),
iterations = iterations + 1,
family = family,
deviance = deviance
)
return (results)
}
set.seed(123)
n <- 200
x <- rnorm(n)
p <- exp(1 + x)/(1 + exp(1 + x))
y <- rbinom(n, 1, p)
# create the design matrix
X <- cbind(1, x)
# fit the model
m1 <- log_test(X, y, "binomial")
# Model output
m1$coefficients
log_test <- function(X, y, family, max_iter = 50){
# Initialize Logistic Regression with the initial value of intercept as listed and all others as 0
y_bar <- mean(y)
initial_intercept <- log(y_bar / (1 - y_bar))
beta_hat <- c(initial_intercept, rep(0, ncol(X) - 1))
# Initialize iterations and previous iterative deviance of initialization
iterations <- 0
prev_deviance <- 0
# Fisher Scoring while loop until convergence or max iterations is reached
while(iterations <= max_iter){
# Update iterations
iterations <- iterations + 1
# Calculate Mu-hat at this iteration
mu_hat <- exp(X %*% beta_hat) / (1 + exp(X %*% beta_hat))
# Calculate the score
U <- t(X) %*% (y - mu_hat)
# Make the diagonal matrix
W <- diag(c(mu_hat * (1 - mu_hat)))
# Update coefficients (beta_hat) with new scores using previous iteration (or initialization)
beta_hat_new <- beta_hat + (solve(t(X) %*% W %*% X) %*% U)
#mu_hat <- exp(X %*% beta_hat_new) / (1 + exp(X %*% beta_hat_new))
# Calculate deviance
deviance <- -2 * sum(dbinom(y, 1, mu_hat, log=T))
# Check to see if convergence was reached
if(abs(deviance - prev_deviance) < 0.001){
break
}
else{
# Reset features for next iteration
prev_deviance <- deviance
beta_hat <- beta_hat_new
}
}
# Calculate standard error of coefficients using the inverse Fisher information matrix
stand_errors <- sqrt(diag(solve(t(X) %*% W %*% X)))
# Calculate the z-statistics for our coefficients
z_stats <- beta_hat_new / stand_errors
# Calculate p-values for coefficients
p_vals <- 2 * pnorm(abs(z_stats), lower.tail = FALSE)
# Set up results DataFrame
results <- list(
coefficients = data.frame(
beta = beta_hat_new,
se = stand_errors,
z_score = z_stats,
p_value = p_vals),
iterations = iterations + 1,
family = family,
deviance = deviance
)
return (results)
}
set.seed(123)
n <- 200
x <- rnorm(n)
p <- exp(1 + x)/(1 + exp(1 + x))
y <- rbinom(n, 1, p)
# create the design matrix
X <- cbind(1, x)
# fit the model
m1 <- log_test(X, y, "binomial")
# Model output
m1$coefficients
14/16
?pt
?read_html
?read_html
library(tidyverse)
library(palmerpenguins)
sub_data <- penguins |>
slice_sample(n=10) |>
select(species, island, bill_length_mm, bill_depth_mm)
sub_data
sub_data |>
count(species, island)
reticulate::repl_python()
sub_data |>
group_by(island, species) |>
summarize(mean_length = mean(bill_length_mm, na.rm=T))
reticulate::repl_python()
sub_data |>
mutate(bill_ratio = bill_length_mm/bill_depth_mm)
reticulate::repl_python()
ex_df <- data.frame(
id = c(1, 2, 3),
x_1 = c(3, 1, 4),
x_2 = c(5, 8, 9),
y_1 = c(0, 1, 2),
y_2 = c(2, 7, 9)
)
ex_df
reticulate::repl_python()
ex_df |>
pivot_longer(cols = -id, names_to = c("group", "obs"), names_sep = "_")
reticulate::repl_python()
ex_df <- data.frame(
id = c(1, 1, 2, 2, 3, 3),
group = c("x", "y", "x", "y", "x", "y"),
value = sample(1:6, replace=T)
)
ex_df
ex_df |>
pivot_wider(id_cols = id, names_from = group, values_from = value)
reticulate::repl_python()
df1 <- data.frame(
id = c(1, 2, 3),
x = c(7, 9, 13)
)
df2 <- data.frame(
id = c(1, 2, 4),
y = c(10, 12, 14)
)
df1 |>
left_join(df2, join_by(id))
df1
df2
reticulate::repl_python()
df1 |>
inner_join(df2, join_by(id))
df1 = data.frame(
a_x = 1,
a_y = 2,
b_x = 2,
b_y = 3
)
df2 = data.frame(
id = c("a", "b"),
z = c(4, 5)
)
df1
df2
df1 |>
pivot_longer(cols = -c(), names_to = c("id", ".value"), names_sep = "_") |>
left_join(df2, join_by(id))
reticulate::repl_python()
